{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/giuseppe/miniconda3/envs/ferret_0.5.0/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/giuseppe/miniconda3/envs/ferret_0.5.0/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "/home/giuseppe/miniconda3/envs/ferret_0.5.0/lib/python3.9/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from IPython.display import display\n",
    "import numpy as np \n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pydub import AudioSegment\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "\n",
    "from ferret import SpeechBenchmark, AOPC_Comprehensiveness_Evaluation_Speech, AOPC_Sufficiency_Evaluation_Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"DynamicSuperb/IntentClassification_FluentSpeechCommands-Action\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:2' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'speakerId', 'transcription', 'audio', 'label', 'instruction'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(DATASET_ID, split=\"test\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'wavs/speakers/Xygv5loxdZtrywr9/77506ae0-452b-11e9-a843-8db76f4b5e29.wav',\n",
       " 'speakerId': 'Xygv5loxdZtrywr9',\n",
       " 'transcription': 'Increase the temperature in the washroom',\n",
       " 'audio': {'path': '77506ae0-452b-11e9-a843-8db76f4b5e29.wav',\n",
       "  'array': array([0.        , 0.        , 0.        , ..., 0.02133179, 0.01977539,\n",
       "         0.01849365]),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 'increase',\n",
       " 'instruction': 'Recognize the action behind the verbal expression. The answer could be activate, bring, change language, deactivate, decrease, or increase.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[0]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are using Wav2Vec2 which expects audio arrays to be in 16kHz. Luckly, this is the native sampling rate of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at superb/wav2vec2-base-superb-ic were not used when initializing Wav2Vec2ForSequenceClassification: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at superb/wav2vec2-base-superb-ic and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## Load model\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "    \"superb/wav2vec2-base-superb-ic\"\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech-XAI: the `SpeechBenchmark` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: if not specified otherwise, `SpeechBenchmark` assumes English as the source language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instantiate benchmark class\n",
    "benchmark = SpeechBenchmark(model, feature_extractor, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from transcribing the example above using WhisperX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(' Increase the temperature in the washroom.',\n",
       " [{'word': 'Increase', 'start': 0.737, 'end': 1.02, 'score': 0.438},\n",
       "  {'word': 'the', 'start': 1.04, 'end': 1.121, 'score': 0.141},\n",
       "  {'word': 'temperature', 'start': 1.141, 'end': 1.526, 'score': 0.444},\n",
       "  {'word': 'in', 'start': 1.546, 'end': 1.627, 'score': 0.848},\n",
       "  {'word': 'the', 'start': 1.647, 'end': 1.728, 'score': 0.953},\n",
       "  {'word': 'washroom.', 'start': 1.768, 'end': 2.132, 'score': 0.588}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, word_timestamps = benchmark.transcribe(\n",
    "    sample[\"audio\"][\"array\"],\n",
    "    current_sr=sample[\"audio\"][\"sampling_rate\"],\n",
    ")\n",
    "text, word_timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain word importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExplanationSpeech(features=['Increase', 'the', 'temperature', 'in', 'the', 'washroom.'], scores=array([[ 0.47325948, -0.45515063, -0.10200211, -0.15734437, -0.12148061,\n",
      "         0.0109534 ],\n",
      "       [ 0.07733697, -0.02064097,  0.34651279, -0.01588559, -0.01463729,\n",
      "        -0.02365428],\n",
      "       [-0.01432282, -0.01848161, -0.00988954, -0.00070852, -0.01123005,\n",
      "         0.32860303]]), explainer='loo_speech+silence', target=[3, 4, 3], audio=<ferret.speechxai_utils.FerretAudio object at 0x7fb7ed858130>)\n"
     ]
    }
   ],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path_or_array=sample[\"audio\"][\"array\"],\n",
    "    current_sr=sample[\"audio\"][\"sampling_rate\"],\n",
    "    methodology='LOO',\n",
    "    word_timestamps=word_timestamps\n",
    ")\n",
    "# display(benchmark.show_table(explanation, decimals=3))\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExplanationSpeech(features=['Increase', 'the', 'temperature', 'in', 'the', 'washroom.'], scores=array([[ 0.30518979, -0.05905298,  0.02406042,  0.06312685, -0.01027066,\n",
      "         0.00634839],\n",
      "       [-0.00192933,  0.04791304,  0.30365684,  0.01351917, -0.02577572,\n",
      "         0.13388124],\n",
      "       [ 0.07868745, -0.02967894,  0.21510287,  0.02970933,  0.03952176,\n",
      "         0.44306288]]), explainer='LIME+silence', target=[3, 4, 3], audio=<ferret.speechxai_utils.FerretAudio object at 0x7fb7ed7f19d0>)\n"
     ]
    }
   ],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path_or_array=sample[\"audio\"][\"array\"],\n",
    "    current_sr=sample[\"audio\"][\"sampling_rate\"], \n",
    "    methodology='LIME',\n",
    "    word_timestamps=word_timestamps\n",
    ")\n",
    "print(explanation)\n",
    "#display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same function but with no word timestamps. The class will generate them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio to get word level timestamps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Transcribed audio with whisperX into:  Increase the temperature in the washroom.\n",
      "ExplanationSpeech(features=['Increase', 'the', 'temperature', 'in', 'the', 'washroom.'], scores=array([[ 2.73476301e-01, -2.75996308e-02,  2.68968859e-02,\n",
      "         4.38230033e-02, -9.83693653e-03,  3.43606501e-02],\n",
      "       [-4.55664511e-02,  2.00727565e-04,  3.07805104e-01,\n",
      "        -7.30904579e-03,  8.18154319e-03,  1.45066594e-01],\n",
      "       [ 7.67946057e-02, -1.63121582e-02,  1.69544374e-01,\n",
      "         1.03233484e-02,  6.95427995e-02,  4.02942428e-01]]), explainer='LIME+silence', target=[3, 4, 3], audio=<ferret.speechxai_utils.FerretAudio object at 0x7fb7ec504c40>, word_timestamps=[{'word': 'Increase', 'start': 0.737, 'end': 1.02, 'score': 0.438}, {'word': 'the', 'start': 1.04, 'end': 1.121, 'score': 0.141}, {'word': 'temperature', 'start': 1.141, 'end': 1.526, 'score': 0.444}, {'word': 'in', 'start': 1.546, 'end': 1.627, 'score': 0.848}, {'word': 'the', 'start': 1.647, 'end': 1.728, 'score': 0.953}, {'word': 'washroom.', 'start': 1.768, 'end': 2.132, 'score': 0.588}])\n"
     ]
    }
   ],
   "source": [
    "explanation = benchmark.explain(\n",
    "    audio_path_or_array=sample[\"audio\"][\"array\"],\n",
    "    current_sr=sample[\"audio\"][\"sampling_rate\"], \n",
    "    methodology='LIME',\n",
    ")\n",
    "print(explanation)\n",
    "#display(benchmark.show_table(explanation, decimals=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EvaluationSpeech(name='aopc_compr_speech', score=[0.32901989901438355, 0.4174739196896553, 0.5148161690682173], target=[3, 4, 3]),\n",
       " EvaluationSpeech(name='aopc_suff', score=[0.17665663920342922, -0.009631142020225525, -0.01769007444381714], target=[3, 4, 3]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aopc_compr = AOPC_Comprehensiveness_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_c = aopc_compr.compute_evaluation(explanation)\n",
    "\n",
    "aopc_suff = AOPC_Sufficiency_Evaluation_Speech(benchmark.model_helper)\n",
    "evaluation_output_s = aopc_suff.compute_evaluation(explanation)\n",
    "\n",
    "evaluation_output_c, evaluation_output_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain paralinguistic impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing audio to get word level timestamps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.2.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "Transcribed audio with whisperX into:  Increase the temperature in the washroom.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_explanation() got an unexpected keyword argument 'audio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explain_table \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_path_or_array\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcurrent_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampling_rate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethodology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mperturb_paraling\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m display(benchmark\u001b[38;5;241m.\u001b[39mshow_table(explain_table, decimals\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "File \u001b[0;32m~/ferret/ferret/benchmark_speech.py:179\u001b[0m, in \u001b[0;36mSpeechBenchmark.explain\u001b[0;34m(self, audio_path_or_array, current_sr, target_class, methodology, perturbation_types, removal_type, aggregation, num_samples, word_timestamps, verbose, verbose_target)\u001b[0m\n\u001b[1;32m    177\u001b[0m     explainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mperturb_paraling\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m perturbation_type \u001b[38;5;129;01min\u001b[39;00m perturbation_types:\n\u001b[0;32m--> 179\u001b[0m         explanation \u001b[38;5;241m=\u001b[39m \u001b[43mexplainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_explanation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m            \u001b[49m\u001b[43maudio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mferret_audio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43mperturbation_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturbation_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m            \u001b[49m\u001b[43mverbose_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose_target\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m         explanations\u001b[38;5;241m.\u001b[39mappend(explanation)\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# table = self.create_table(importances)\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m## Get the importance of each word\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# elif:\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_explanation() got an unexpected keyword argument 'audio'"
     ]
    }
   ],
   "source": [
    "explain_table = benchmark.explain(\n",
    "    audio_path_or_array=sample[\"audio\"][\"array\"],\n",
    "    current_sr=sample[\"audio\"][\"sampling_rate\"],\n",
    "    methodology='perturb_paraling',\n",
    ")\n",
    "display(benchmark.show_table(explain_table, decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbation_types = ['time stretching', 'pitch shifting', 'reverberation', 'noise']\n",
    "variations_table = benchmark.explain_variations(\n",
    "    audio_path=audio_path,\n",
    "    perturbation_types=perturbation_types\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations_table_plot = {k:variations_table[k] for k in variations_table if k in ['time stretching', 'pitch shifting', 'noise']}\n",
    "fig = benchmark.plot_variations(variations_table_plot, show_diff = True, figsize=(4.6, 4.2));\n",
    "# fig.savefig(f'example_{dataset_name}_context.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SUPERB - IC Task (FSC).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
